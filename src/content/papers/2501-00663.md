---
type: ResearchPaper
title: Titans - Learning to Memorize at Test Time
description: Titans introduce a neural long-term memory module that learns to memorize historical context, enabling attention to focus on the current context while using long past information. The architecture scales to very long context with strong performance on language modeling and needle-in-haystack tasks.
tags:
  - AI
  - Machine Learning
  - NLP
publishedDate: "Jan 1, 2025"
sourceUrl: https://arxiv.org/abs/2501.00663
sourceLabel: arXiv
reviewedDate: "Jan 2025"
dateOfReview: "Jan 2025"
status: Complete
---

> Titan is a family of neural architectures that combines short-term memory (via attention) and long-term memory (via a neural memory module) to address the limitations of traditional recurrent models and attention mechanisms. It is specifically designed to handle long-context tasks effectively while maintaining fast and parallelizable training and inference.

## Limitations of Traditional Sequence Models

**Computational Complexity**

- **Transformers:** Attention-based models excel in sequence modeling by learning from context and scaling efficiently. Attention modules function as associative memory, retrieving information based on query-key similarities. However, they are computationally expensive, with time and memory growing **quadratically** with context length. Real-world tasks like language modeling, video understanding, and time series forecasting often involve very large contexts.
- **Linear Transformers:** They replace the softmax attention with kernel functions, significantly reducing memory. While more efficient for longer contexts, they compress data into matrix-valued states, limiting performance compared to standard Transformers. Linear models handle long contexts efficiently but struggle to compress context effectively.

**Memory**

Memory plays a crucial role in learning and decision-making. RNNs use vector-based memory updated and retrieved based on input sequences. Transformers maintain memory by adding key-value pairs retrieved by comparing queries with keys; they retain all historical data. Linear Transformers compress this data into matrices. Storing memory in a simplified form (single vector or matrix) may be insufficient for long-term storageâ€”a deeper, more advanced memory structure may be needed. Specialized and interconnected memory components that store, update, and retrieve information across short-term and long-term contexts can handle large, complex datasets over extended periods.

## A Mathematical Perspective

### Transformers (Attention Mechanism)

Given an input matrix $\mathbf{x} \in \mathbb{R}^{N \times d_{\text{in}}}$, the output $\mathbf{y}$ is computed using $Q = \mathbf{x} W_Q$, $K = \mathbf{x} W_K$, $V = \mathbf{x} W_V$, and:

$$y_i = \sum_{j=1}^i \frac{\exp(Q_i^\top K_j / \sqrt{d_{\text{in}}})}{\sum_{\ell=1}^i \exp(Q_i^\top K_\ell / \sqrt{d_{\text{in}}})} V_j$$

Transformers need at least $N \times d$ operators to compute the output, resulting in larger memory consumption and lower throughput for longer sequences.

### Linear Attention

In linear attention, softmax is replaced with a kernel function $\phi(\cdot, \cdot)$ satisfying $\phi(x, y) = \phi(x) \phi(y)$ (e.g. $\phi$ a feature map). This yields forms like:

$$y_i = \frac{\phi(Q_i^\top)\sum_{j=1}^i \phi(K_j)V_j}{\phi(Q_i^\top) \sum_{l=1}^i \phi(K_l)}$$

So we have two recurrent summations: $\sum_{j=1}^i \phi(K_j)V_j$ and $\sum_{j=1}^i \phi(K_j)$, which can be updated incrementally and reduce complexity, but compress context into a single state and limit expressiveness.

## Titan Architecture

Titan combines **short-term memory** (attention over a limited context window) with **long-term memory** (a neural memory module that learns to memorize and retrieve historical information). Attention can then focus on the current context while the memory module provides access to long past information. This design scales to very long context with parallelizable training and inference.

![Titan architecture](/papers/2501-00663/Titans_Learning%20to%20Memorize%20at%20Test%20Time_2.png)

Key components include: a **memory module** that learns to write and read from a learned memory state; **attention** restricted to a local or truncated context for efficiency; and **integration** of memory readouts with the current representation so the model can use both recent and memorized information.

![Memory and attention](/papers/2501-00663/Titans_Learning%20to%20Memorize%20at%20Test%20Time_3.png)

## Experiments and Key Findings

Experimental results on **language modeling**, **common-sense reasoning**, **genomics**, and **time series** show that Titans are more effective than Transformers and modern linear recurrent models. The architecture **scales to over 2M context** with higher accuracy on **needle-in-haystack** tasks, demonstrating that the neural long-term memory successfully memorizes and retrieves information from the distant past.

![Scaling and needle-in-haystack](/papers/2501-00663/Titans_Learning%20to%20Memorize%20at%20Test%20Time_4.png)

**Summary:** Titans address the limitations of both full attention (quadratic cost) and linear/compressive recurrence (limited long-term recall) by adding a dedicated neural memory that learns to memorize at test time. This enables long-context modeling with strong performance on tasks that require retrieving specific information from very long sequences.
