---
type: ResearchPaper
title: CoRAG - Chain of Retrieval Augmented Generation
description: CoRAG trains o1-like RAG models that retrieve and reason over information step by step before generating the final answer. It uses rejection sampling to generate intermediate retrieval chains and achieves strong gains on multi-hop QA and state-of-the-art on the KILT benchmark.
tags:
  - AI
  - Machine Learning
  - NLP
publishedDate: "Jan 1, 2025"
sourceUrl: https://arxiv.org/abs/2501.14342
sourceLabel: arXiv
reviewedDate: "Jan 2025"
dateOfReview: "Jan 2025"
status: Complete
---

Most RAG systems use a two-step process: first retrieving information, then passing it to the language model for generation. The quality of retrieved data is crucial, and retrieval models are optimized for speed to handle large datasets.

Dense retrievers, which use a bi-encoder architecture, encode documents and queries into vectors, enabling fast search but limiting their ability to handle complex queries. Multi-hop reasoning tasks are particularly challenging, as they require retrieving multiple pieces of information step by step.

## Dynamic Retrieval Framework

To improve retrieval quality, this paper proposes a new dynamic retrieval framework that adjusts retrieval steps based on the evolving state of the reasoning process. At test time, the model can modify its retrieval strategy, similar to how humans search for information iteratively to solve complex problems. Instead of relying solely on in-context learning or distillation from proprietary models, the authors explicitly train language models to retrieve step by step.

## How CoRAG Works

### Training

The model is trained using rejection sampling, which generates intermediate retrieval chains to enrich existing RAG datasets. Open-source models are then fine-tuned on these datasets using next-token prediction.

### Testing

Various decoding strategies are used to control computational cost, including:

- **Greedy decoding**: Choosing the most probable token at each step.
- **Best-of-N sampling**: Selecting the best response from multiple generated outputs.
- **Tree search**: Exploring different retrieval paths before choosing the best one.

**Core Insights**

- **Significant performance gains** in multi-hop QA tasks, where retrieval models often struggle to recall all necessary information in one step.
- **A log-linear relationship** between token usage and model performance, meaning better answers require more tokens, but efficiency varies across datasets.
- **New state-of-the-art scores** on the KILT benchmark, which covers a broad range of knowledge-intensive tasks.
- **Task-dependent scaling behavior**: Some datasets (e.g., NQ) already have strong retrievers, so additional retrieval steps bring little benefit. This suggests that compute resources should be allocated dynamically based on query complexity.
- **Improved robustness**: CoRAG effectively reforms complex queries and adapts to retrieval models of different quality, reducing errors caused by poor retrieval results.

## Advancements in Multi-Step Retrieval

The effectiveness of RAG depends on retrieving high-quality, relevant information. Many recent studies have focused on improving general-purpose text embeddings, but these embeddings face challenges with complex queries since they rely on fixed-size vector representations for efficiency.

To overcome the limitation of a single retrieval step followed by generation, recent research has developed multi-step iterative retrieval and generation methods:

- **FLARE**: Lets an LLM decide dynamically when and what to retrieve while generating responses.
- **ITER-RETGEN**: Alternates between retrieval-augmented generation and generation-augmented retrieval, improving multi-step question answering.
- **IRCoT**: Uses a chain-of-thought (CoT) approach to iteratively refine retrieval.
- **Self-RAG**: Allows LLMs to retrieve, generate, and self-critique to enhance factual accuracy and citation precision.
- **Auto-RAG**: Uses heuristics and answer matching for intermediate retrieval steps but underperforms compared to state-of-the-art models.

Rather than relying on few-shot prompting or knowledge distillation from proprietary models, this study proposes explicitly training LLMs to retrieve and reason iteratively.

## Scaling Test-Time Computation

Instead of generating a final answer directly, Chain-of-Thought (CoT) improves reasoning by encouraging step-by-step thinking. Tree-of-Thought (ToT) extends this idea by using a tree structure to explore different reasoning paths. STaR uses bootstrapping techniques to generate intermediate reasoning steps for training. OpenAI's o1 model applies large-scale reinforcement learning for reasoning tasks, though its details are undisclosed. However, these methods increase token consumption, leading to longer response times.

In RAG, test-time computation can be scaled by retrieving more documents or performing additional retrieval steps. Some recent methods include:

- **LongRAG**: Uses long-context LLMs with more retrieved documents to enhance performance.
- **IterDRAG**: Examines test-time scaling laws through few-shot prompting and iterative retrieval, processing up to 5M tokens.
- **Search-o1**: Combines the open-source QwQ model with Bing search for better knowledge-intensive task performance.

This study extends test-time scaling in RAG by fine-tuning models for targeted retrieval while exploring different decoding strategies.

---

## CoRAG Framework Overview

![CoRAG framework](/papers/2501-14342/CoRAG_Chain%20of%20Retrieval%20Augmented%20Generation_0.png)

The **CoRAG** framework (illustrated in Figure 1) has three main components:

1. **Retrieval chain generation** using rejection sampling
2. **Model training** with augmented datasets
3. **Scaling test-time computation**

### Retrieval Chain Generation

Most **Retrieval-Augmented Generation (RAG)** datasets only provide a query $Q$ and the final answer $A$, without showing intermediate retrieval steps. To address this, we introduce an automated method that generates **retrieval chains** using **rejection sampling**.

Each retrieval chain consists of a sequence of **sub-queries** and their corresponding **sub-answers**:

$Q_{1:L} = \{Q_1, Q_2, \dots, Q_L\}, \quad A_{1:L} = \{A_1, A_2, \dots, A_L\}$

where $L$ is the maximum length of the chain.

To generate a sub-query $Q_i$, we use an LLM:

$Q_i = \text{LLM}(Q_{<i}, A_{<i}, Q)$

where $Q_{<i}$ and $A_{<i}$ are the previous sub-queries and sub-answers.

Next, we retrieve the **top-k** most relevant documents $D_{1:k}^{(i)}$ using a retriever with $Q_i$ as the search query. The LLM then generates the sub-answer:

$A_i = \text{LLM}(Q_i, D_{1:k}^{(i)})$

This process repeats until either the chain reaches the maximum length $L$, or the generated answer $A_i$ matches the correct answer $A$.

To evaluate a retrieval chain, we compute the **log-likelihood** of the correct answer given the chain:

$\log P(A | Q, Q_{1:L}, A_{1:L})$

The retrieval chain with the highest log-likelihood score is chosen to **augment** the dataset.

### Model Training

Each training example in the **augmented dataset** is represented as a tuple:

$(Q, A, Q_{1:L}, A_{1:L})$

along with the **top-k** retrieved documents for the query $Q$ and each sub-query.

We fine-tune an **LLM** using **next-token prediction** in a **multi-task learning** setup. The model learns three tasks simultaneously:

1. **Predicting the next sub-query:** $L_{\text{sub\_query}} = -\log P(Q_i | Q, Q_{<i}, A_{<i}), \quad i \in [1, L]$
2. **Predicting the sub-answer:** $L_{\text{sub\_answer}} = -\log P(A_i | Q_i, D_{1:k}^{(i)}), \quad i \in [1, L]$
3. **Predicting the final answer:** $L_{\text{final\_answer}} = -\log P(A | Q, Q_{1:L}, A_{1:L}, D_{1:k})$

The model **only computes cross-entropy loss** for the target output tokens. Since the same prompt templates are used for both data generation and training, the **fine-tuned model can be used for the next round of rejection sampling**, allowing for iterative improvements.

---

## Test-Time Scaling

Once the **CoRAG model** is trained, different **decoding strategies** can be used to balance **performance** and **compute cost** during inference.

**Measuring Compute Cost**

We measure test-time compute by counting the **total number of tokens used**, including **prompt tokens** (input given to the model) and **generated tokens** (model's output). Unlike some previous methods that consider only one type of token, we **count both**. Even though **prompt tokens** are cheaper (due to caching and parallel computation in the **prefilling stage**), we treat them **equally** for simplicity.

### Decoding Strategies

**1. Greedy Decoding**

- The model **generates L sub-queries and sub-answers sequentially**, without randomness.
- The final answer is generated using the same prompt template as in training.
- **Pros:** Fast and deterministic. **Cons:** May not always find the best retrieval chain.

**2. Best-of-N Sampling**

- The model **samples N different retrieval chains** using a randomness level (**temperature = 0.7**).
- It then selects the **best chain** to generate the final answer.
- **Pros:** More robust than greedy decoding. **Cons:** More computationally expensive due to multiple samples.

**3. Tree Search (Breadth-First Search - BFS)**

- Expands multiple **sub-query states** at each step.
- For each expanded state, multiple **retrieval rollouts** are performed.
- The **average penalty score** of these rollouts is computed, and the state with the **lowest score** is selected for further expansion.
- **Pros:** Explores multiple retrieval paths. **Cons:** Requires significantly more computation and memory.

The amount of computation can be adjusted using different parameters: for all strategies, limit the **retrieval chain length** $L$; for Best-of-N Sampling, change the **number of sampled chains** $N$; for Tree Search, adjust the **number of rollouts** and **expansion size**.

CoRAG-8B outperforms baseline models in multi-hop QA tasks, achieving the highest exact match (EM) and F1 scores across most datasets except for Bamboogle, where retrieval limitations impact performance. In the KILT benchmark, CoRAG-8B sets a new state-of-the-art across all tasks except for FEVER, where it narrowly falls behind a larger model. Scaling test-time compute by increasing retrieval chain length improves performance significantly at lower values but yields diminishing returns at higher lengths.

## Key Findings

**Iterative Rejection Sampling:** CoRAG trains itself iteratively, similar to LLM post-training. Results show improvement on 2WikiMultihopQA but slight declines on other datasets, indicating that instruction-tuned LLMs already perform well in retrieval chain generation.

**Robustness and Generalization:** Performance improves when using stronger retrievers. Using weaker LLMs (e.g., Llama-3B) for retrieval and stronger LLMs (Llama-8B) for training can reduce computational cost while maintaining performance (weak-to-strong generalization).

**Effectiveness of Chain-of-Retrieval:** Multi-hop QA datasets benefit significantly from the retrieval chain mechanism. For simpler tasks (e.g., NQ, TriviaQA), the advantage is marginal, suggesting the need for adaptive decoding strategies.

**Learning to Stop at Test Time:** A variant of CoRAG learns when to stop retrieval instead of always performing a fixed number of steps. While this saves computational cost, it slightly reduces performance, making it a trade-off between efficiency and accuracy.

CoRAG achieves state-of-the-art results on multi-hop QA datasets and the KILT benchmark, surpassing larger LLM-based baselines. Future work aims to extend its capabilities to more complex and economically valuable RAG tasks, improving factual consistency in AI systems.
