---
type: ResearchPaper
title: 'Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows'
tags: [AI, Text-to-SQL, NL2SQL, NL2SQL Evaluation]
publishedDate: '2025-03-17'
sourceUrl: https://arxiv.org/abs/2411.07763
sourceLabel: arXiv
plannedDateOfReview: '2026-02-09'
dateOfReview: '2025-05-09'
status: Completed
---

# Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows

## Why This Paper Was Introduced?

- **Text-to-SQL as a bridge between humans and databases**: The introduction frames text-to-SQL as a core technology for helping users interact with relational databases and automate analytics workflows; prior benchmarks assume this bridge is adequately evaluated using small, simplified databases and single-query outputs, which the authors argue fails to reflect how text-to-SQL is actually used in real enterprise settings with complex workflows and data engineering steps

- **Performance of LLMs on existing text-to-SQL benchmarks**: The paper characterizes prior work as demonstrating very high execution accuracy on benchmarks like Spider 1.0 and BIRD, suggesting strong code generation capability; the authors argue this apparent success is misleading because these benchmarks rely on non-industrial databases with few tables, few columns, and simplified SQL that does not capture real-world complexity

- **Database scale and schema complexity**: In the introduction, enterprise databases are described as having thousands of columns, nested structures, and multiple schemas across cloud and local systems; prior datasets are characterized as using lightweight schemas, which the authors claim makes schema linking and reasoning unrealistically easy compared to industrial databases

- **SQL dialect diversity**: The paper introduces real-world SQL workflows as spanning many database systems, each with distinct dialects and functions; related work and prior benchmarks are described as largely ignoring dialect diversity, implicitly assuming a narrow or uniform SQL syntax that does not transfer to enterprise environments

- **Workflow-level SQL usage**: The introduction frames enterprise text-to-SQL as involving multi-step workflows including data wrangling, transformation, and analytics; prior work is formulated primarily as single-shot semantic parsing from text to one SQL query, which the authors argue omits the procedural and iterative nature of real data workflows

- **Use of external context and documentation**: The paper characterizes real SQL work as requiring consultation of database metadata, SQL documentation, and project-level context; prior benchmarks are described as providing all necessary information directly in the prompt, making them insufficient for evaluating whether models can ground SQL generation in external knowledge sources

- **Agentic interaction with execution environments**: In the introduction, real-world SQL development is framed as interactive, involving execution feedback and iterative refinement; related text-to-SQL benchmarks are characterized as static input–output tasks, which the authors claim cannot evaluate models’ ability to operate as autonomous agents in realistic environments

- **Scope of prior code generation benchmarks**: The related work situates text-to-SQL benchmarks alongside general code generation datasets that treat code synthesis as sequence-to-sequence prediction; the authors argue these formulations are insufficient for enterprise SQL because they abstract away environment interaction, repository context, and long-horizon reasoning

- **Gap between research benchmarks and industrial practice**: The paper explicitly frames a gap between academic text-to-SQL research and enterprise usage, characterizing existing benchmarks as failing to align with industrial data scale, tooling, and workflows; this misalignment is presented as the core motivation for introducing a new benchmark that better reflects real-world requirements.

## What this paper introduces?

- **Spider 2.0 benchmark**: The paper formally introduces *Spider 2.0* as an evaluation framework consisting of 632 real-world enterprise text-to-SQL workflow problems, defined by a tuple of a natural language question, a database interface, and a project-level codebase with documentation; this differs from prior benchmarks by framing the task as an interactive workflow over real industrial databases rather than a static mapping from text to a single SQL query

![image](/papers/2411-07763/image.png)

- **Agentic text-to-SQL task formulation**: Spider 2.0 is explicitly defined as a *code agent task* where an agent iteratively modifies and executes SQL or Python code against a database until a final observation is produced as the answer; this generalizes prior text-to-SQL formulations that define the task as producing one SQL string without interaction or intermediate execution

- **Success Rate (SR) metric for workflow tasks**: The paper introduces *Success Rate (SR)* as the evaluation criterion for Spider 2.0, defined as the proportion of task instances successfully completed based on human-written evaluation scripts over final observations; this differs from prior execution accuracy metrics that compare only the result of a single SQL query

- **Spider 2.0-lite setting**: The paper defines *Spider 2.0-lite* as a self-contained text-to-SQL task with preprocessed database schema and documentation, where the output is restricted to a single SQL query; this reframes the same underlying data into a traditional parser setting while removing codebase interaction, which prior benchmarks did not explicitly separate from agentic workflows

- **Spider 2.0-snow setting**: The paper introduces *Spider 2.0-snow* as a variant of Spider 2.0-lite entirely hosted on Snowflake, formally fixing the database system and dialect; this differs from prior datasets by isolating dialect-specific SQL generation within a controlled enterprise warehouse environment

- **Enterprise-scale database characterization**: Spider 2.0 databases are formally characterized as real industrial datasets with an average of hundreds of columns, nested schemas, multiple schemas, and terabyte-scale data volumes across systems such as BigQuery, Snowflake, SQLite, DuckDB, PostgreSQL, and ClickHouse; this contrasts with prior benchmarks that define databases as small, single-schema, and local

- **Inclusion of project-level data transformation tasks**: The paper introduces DBT-based project tasks as first-class benchmark instances, defined by incomplete real-world transformation codebases that must be modified to satisfy task instructions; this extends prior text-to-SQL benchmarks that do not define tasks over multi-file data engineering projects

- **Execution-based focused evaluation**: The paper defines an execution-based focused evaluation protocol that scores outputs by checking whether essential columns or values in the gold result appear in the predicted result, rather than requiring exact table matching; this differs from prior execution accuracy definitions that assume fully specified outputs

- **Spider-Agent baseline framework**: The paper introduces *Spider-Agent* as a database-oriented agent framework with a fixed action space for interacting with SQL databases, file systems, and execution environments; this differs from prior general-purpose code agents by being formally scoped to database and SQL workflow interactions

- **Unified benchmark spanning agentic and parser settings**: The paper introduces a single benchmark family (Spider 2.0, Spider 2.0-lite, Spider 2.0-snow) defined over the same underlying data sources but different task formulations and output constraints; this differs from prior work where agent-based and text-to-SQL evaluations are conducted on disjoint datasets

## The Implementation - Benchmark Construction

### **Benchmark Construction and Task Instantiation**

- **Database collection procedure**:

    - Inputs: Public and enterprise-grade databases from BigQuery, Snowflake, SQLite, DuckDB, PostgreSQL, ClickHouse.

    - Constraint: Each database must contain more than 200 columns or include nested schema structures.

    - Output: A filtered set of databases used as execution backends for all benchmark tasks.

    ![image](/papers/2411-07763/image%20(1).png)

    ![image](/papers/2411-07763/image%20(2).png)

- **SQL and project sourcing**:

    - Inputs: Real SQL queries from tutorials, forums, and open-source projects; DBT projects from Fivetran and DBT repositories.

    - Constraint: SQL queries must exceed 50 whitespace-tokenized tokens and originate from real usage.

    - Output: A pool of complex SQL queries and multi-file DBT project codebases.

- **SQL rewriting process**:

    - Inputs: Original SQL queries.

    - Operations:

        - Surface-level rewriting: Modify parameters, output formats, or filtering conditions.

        - Semantic-level rewriting: Expand or alter task intent, merge multiple SQLs, or add advanced requirements.

    - Constraint: Rewritten SQL must execute successfully and return non-empty results.

    - Output: Gold SQL queries used as reference implementations.

- **Context assembly**:

    - Inputs: SQL queries, databases, external documents, and codebases.

    - Operations:

        - Collect SQL dialect documentation for required functions.

        - Preserve original project codebases for agentic tasks.

        - Attach auxiliary files (e.g., YAML schemas, markdown docs, predefined answer files).

    - Output: Complete task environments for Spider 2.0 and self-contained inputs for Spider 2.0-lite/snow.

- **Natural language instruction annotation**:

    - Inputs: SQL queries and contextual materials.

    - Operations:

        - Write natural language task descriptions aligned with SQL semantics.

        - Produce two variants when applicable: natural (Spider 2.0) and unambiguous (Spider 2.0-lite/snow).

    - Output: Task instructions paired with environments or schemas.

---

### **Task Formulation and Execution**

![image](/papers/2411-07763/image%20(3).png)

- **Agentic task execution loop (Spider 2.0)**:

    - Inputs: Question Q, database interface I, codebase C.

    - Procedure:

        - Initialize codebase state.

        - Iteratively apply actions that modify or execute code.

        - After each action, obtain observation O_k = \text{execute}(C, I, Q).

    - Termination condition: Agent emits a Terminate action or repeats identical outputs.

    - Output: Final observation O_k treated as the task answer.

- **Text-to-SQL parsing task (Spider 2.0-lite / snow)**:

    - Inputs: Natural language question Q, database schema D, auxiliary documentation E.

    - Procedure: Apply a parser f(Q, D, E \mid \theta) to produce a single SQL query.

    - Output: SQL string executed against the database.

---

### **Spider-Agent Framework Implementation**

- **Action space definition**:

    - Actions include: Bash, CreateFile, EditFile, ExecuteSQL, GetTables, GetTabInfo, SampleRows, FAIL, Terminate.

    - Constraint: Only predefined actions are allowed; all operations are non-interactive.

- **ReAct-style interaction protocol**:

    - Each step consists of:

        - Thought: Model-generated reasoning text.

        - Action: One action invocation with parameters.

        - Observation: Environment-provided output.

    - Constraint: Maximum of 30 steps per task; terminate on repeated outputs or timeouts.

- **SQL execution handling**:

    - Inputs: SQL query, execution mode (print or save), save path.

    - Operations:

        - Execute query on the target database system.

        - Capture execution errors or result tables.

    - Output: Printed results or CSV files stored under /workspace.

---

### **Evaluation Protocols**

- **Execution Accuracy (EX) for Spider 2.0-lite/snow**:

    - Inputs: Execution results of predicted SQL $\hat{v}_n$ and gold SQL v_n.

    - Criterion:

        - Score 1 if all columns in v_n appear in \hat{v}_n.

        - Score 0 otherwise.

    - Output: Mean accuracy over all test instances.

- **Success Rate (SR) for Spider 2.0**:

    - Inputs: Final agent outputs (string, table, or database).

    - Procedure:

        - Apply human-written evaluation scripts specific to output type.

        - Scripts perform string matching, numeric tolerance checks, table column checks, or database comparisons.

    - Output: Binary success indicator per task, averaged across tasks.

- **Table-based evaluation mechanism**:

    - Inputs: Predicted table, gold table(s).

    - Parameters:

        - condition_cols: Column indices to be matched.

        - ignore_order: Boolean for row order sensitivity.

    - Output: Pass/fail decision based on column-wise containment.

---

### **Experimental Execution Setup**

- **Model inference configuration**:

    - Parameters: Temperature = 0 or 1.0 depending on setting; top-p = 0.9 for agents.

    - Constraint: Input prompts truncated from the beginning if exceeding model context limits.

- **Few-shot and oracle settings**:

    - Few-shot: Manually selected demonstrations included in prompts.

    - Oracle functions: Relevant SQL function documentation injected directly into prompts.

    - Output: Modified prompts used for SQL generation.

---

### **Cost and Runtime Controls**

- **Timeout and repetition guards**:

    - Constraint: Any action exceeding 120 seconds is aborted.

    - Constraint: Agent terminates after producing identical outputs three times consecutively.

- **Result persistence**:

    - Constraint: Final answers must be saved as CSV files or emitted as text, not as SQL code alone.

## Experiments & Results

- **Evaluation of LLM agents on Spider 2.0-lite and Spider 2.0-snow**: The paper evaluates multiple LLMs using the Spider-Agent framework on the Spider 2.0-lite and Spider 2.0-snow benchmarks; the reported metric is Execution Accuracy (EX); the highest overall EX is achieved by o1-preview with 23.22% on Spider 2.0-lite and 23.77% on Spider 2.0-snow, with performance decreasing from Easy to Hard subsets; results are reported in Table 4

![image](/papers/2411-07763/image%20(4).png)

- **Comparison with traditional text-to-SQL methods**: The paper evaluates DIN-SQL, DAIL-SQL, CHESS, and SFT CodeS on Spider 1.0, BIRD, Spider 2.0-lite, and Spider 2.0-snow; the metric is Execution Accuracy (EX); methods that achieve over 80% EX on Spider 1.0 and over 50% on BIRD achieve at most 5.68% EX on Spider 2.0-lite and 2.20% EX on Spider 2.0-snow; results are reported in Table 5

![image](/papers/2411-07763/image%20(5).png)

- **Agent framework comparison on Spider 2.0**: The paper compares AutoEval, Reflexion, CodeR, and Spider-Agent on the full Spider 2.0 benchmark; the metric is Success Rate (SR); Spider-Agent with o1-preview achieves the highest SR at 21.36%, while other frameworks with GPT-4o achieve SRs below 8%; results are reported in Table 6

- **Performance by SQL difficulty level**: The paper reports Execution Accuracy for Easy, Medium, and Hard subsets of Spider 2.0-lite and Spider 2.0-snow; EX decreases monotonically as SQL token length increases, with Hard cases consistently below 16% EX for all evaluated models; results are reported in Table 4

- **Effect of nested schema on agent performance**: The paper evaluates Spider-Agent performance on Spider 2.0 tasks with and without nested columns; the metric is Success Rate (SR); tasks with nested columns achieve 10.34% SR compared to 27.38% SR without nested columns; results are reported in Table 7

- **Effect of external documentation requirements**: The paper evaluates Spider-Agent performance on tasks that require external documents versus those that do not; the metric is Success Rate (SR); tasks requiring external documents achieve 11.54% SR compared to 26.64% SR for tasks without external documents; results are reported in Table 8

- **Performance on DBT project tasks**: The paper evaluates Spider-Agent on DBT-based project tasks versus non-DBT tasks within Spider 2.0; the metric is Success Rate (SR); DBT project tasks achieve 12.82% SR compared to 23.22% SR for non-DBT tasks; results are reported in Table 9

- **Error category distribution**: The paper analyzes 300 sampled failure cases from Spider 2.0 and Spider 2.0-lite; error types are categorized and reported as proportions; erroneous data analysis accounts for 35.5% of errors, wrong schema linking for 27.6%, and join errors for 8.3%; results are summarized in Figure 4

- **Oracle SQL function documentation experiment**: The paper evaluates DAIL-SQL under settings with and without oracle-provided SQL function documentation on Spider 2.0-lite; the metric is Execution Accuracy (EX); EX increases from 5.68% to 9.51% when using o1-preview with oracle functions; results are reported in Table 10

- **Few-shot prompting experiment**: The paper evaluates DAIL-SQL with 0-shot, 1-shot, and 3-shot prompting on Spider 2.0-lite; the metric is Execution Accuracy (EX); EX increases marginally from 5.68% (0-shot) to 6.76% (3-shot); results are reported in Table 11

- **Performance across database systems**: The paper reports Success Rate (SR) on Spider 2.0 grouped by database type; BigQuery tasks achieve 24.07% SR, Snowflake 7.14%, SQLite 20.74%, DuckDB 17.69%, and ClickHouse 57.14% (with small sample size); results are reported in Table 20

- **Agent action statistics**: The paper reports statistics over successful Spider-Agent trajectories; the average number of steps per solved task is 9.0, with a maximum of 17; action frequency by step is visualized; results are reported in Figure 20

- **Cost analysis of methods**: The paper reports average API cost per instance for different methods; Spider-Agent with o1-preview averages $0.75 per instance, while DAIL-SQL + GPT-4o averages $0.09; results are reported in Table 21

## When this can be applied?

1. Holds only when evaluation is done with the paper’s “Success Rate (SR)” criterion for Spider 2.0 (binary 0/1 per example from an evaluation script)  .

2. Holds only when evaluation is done with “Execution Accuracy (EX)” for Spider 2.0-lite / Spider 2.0-snow (output must be SQL; compare executed results)  .

3. Holds only when the “execution-based focused evaluation” is used (evaluation scripts focus on “essential components” and ignore “irrelevant/non-essential columns” when questions don’t specify return columns)  .

4. Holds only under the paper’s difficulty regime definition (difficulty is determined by tokenized gold-SQL length: <80 Easy, 80–159 Medium, ≥160 Hard)  .

5. Holds only when prompts fit the model’s max token limit or are handled by truncation-from-the-beginning when exceeding limits (the paper states truncation is applied in experiments)  .

6. Fails when the “input prompt exceeds the LLM’s maximum length,” because truncation can make the needed information “inaccessible” (listed as an error category)  .

7. Holds only under the paper’s reproducibility setting where temperature is fixed at 0.0 (stated for experiments / prompt-based methods)  .

8. Holds only under the Spider-Agent run constraints where the agent is “heuristically” requested to finish within a maximum step limit of 30  .

9. Fails (terminates early) when the agent outputs the same result three times in a row, because the framework “automatically terminates” in that condition  .

10. Fails (terminates early) when any action takes longer than 120 seconds, because the framework “automatically terminates” under that timeout  .

11. Fails more often in the task regime “with nested columns” (nested/array/dict-like fields): reported success rate is much lower for tasks with nested columns than without them (Tab. 7)  .

12. Fails more often in the task regime “with external documents”: reported success rate is much lower for tasks requiring external documents than tasks without them (Tab. 8)  .

13. Fails more often in the task regime “with DBT projects” (project-level workflows): reported success rate is low on DBT project tasks compared to non-DBT tasks (Tab. 9)  .

