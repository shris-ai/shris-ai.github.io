---
type: ResearchPaper
title: DSPY - Compiling Declarative Language Model Calls into Self-Improving Pipelines
description: DSPy abstracts LM pipelines as text transformation graphs with declarative modules that can learn prompting, finetuning, and reasoning. A compiler optimizes any DSPy pipeline for a given metric; succinct programs outperform few-shot and expert demonstrations.
tags:
  - AI
  - Machine Learning
  - NLP
publishedDate: "Oct 1, 2023"
sourceUrl: https://arxiv.org/abs/2310.03714
sourceLabel: arXiv
reviewedDate: "Jan 2025"
dateOfReview: "Jan 2025"
status: Complete
---

> The ML community is actively exploring techniques for prompting language models (LMs) and creating pipelines to solve complex tasks. However, existing LM pipelines often rely on hard-coded prompt templates, which are tedious and error-prone. To address this, **DSPy** introduces a systematic approach by representing LM pipelines as **text transformation graphs** (imperative computational graphs with declarative modules for LMs).

## Key Features of DSPy

1. **Parameterization:** Modules in DSPy can learn to apply prompting, fine-tuning, augmentation, and reasoning techniques.
2. **Optimization Compiler:** DSPy includes a compiler that optimizes pipelines to maximize specific metrics by generating and collecting demonstrations.
3. **Expressive Pipelines:** DSPy enables succinct programs to express and optimize pipelines for tasks like solving math word problems, multi-hop retrieval, complex question answering, and controlling agent loops.
4. **Performance:** Within minutes of compiling, DSPy produces pipelines that outperform standard few-shot prompting and expert-crafted demonstrations, even for smaller models like T5 and Llama2-13b-chat.
5. **Scalability:** DSPy programs for smaller, open-source models are competitive with solutions that rely on larger, proprietary LMs like GPT-3.5.

## Impact

DSPy simplifies and enhances the development of LM pipelines, making them more efficient and accessible. It is available at [GitHub: stanfordnlp/dspy](https://github.com/stanfordnlp/dspy).

## Introduction

Interest is growing in multi-stage pipelines that decompose complex tasks into manageable LM calls, improving performance. However, the reliance on hard-coded prompt templates—long, handcrafted strings created via trial and error—makes existing LM pipelines brittle and unscalable. These templates are sensitive to task requirements and fail to generalize across LMs, domains, or inputs. To address these limitations, DSPy introduces a novel programming model for building and optimizing LM pipelines systematically.

## DSPy: A Systematic Approach to LM Pipelines

### Key Innovations

1. **Modular Programming:** DSPy abstracts LM pipelines as **text transformation graphs**, akin to neural network architectures. It replaces string-based prompting (e.g., Chain of Thought or ReAct techniques) with **declarative modules** that include natural-language-typed signatures. Each module represents a task-specific transformation, such as summarization or question answering.
2. **Parameterization:** Modules are parameterized to learn their desired behavior by iteratively bootstrapping demonstrations within the pipeline. This eliminates manual prompt tuning.
3. **Define-by-Run Graphs:** Inspired by PyTorch, DSPy uses computational graphs to flexibly connect modules through logical control flows, such as `if` statements, loops, and exceptions.
4. **Compiler Optimization:** DSPy introduces a **compiler** that optimizes pipelines to improve quality or cost. Inputs: the program, a few labeled training inputs, and a validation metric. Outputs: optimized few-shot prompts or finetuned small LMs for pipeline steps. **Teleprompters** are general-purpose optimization strategies that guide how modules learn from data.

### Case Studies and Performance

**Evaluation Domains:** Math word problems (GSM8K), multi-hop question answering (HotPotQA). Techniques explored: Chain of Thought (CoT) prompting, multi-chain reflection, multi-hop retrieval, retrieval-augmented question answering, agent loops.

**Results:** DSPy programs outperform systems relying on hand-crafted prompts. Smaller, efficient LMs like Llama2-13b are used effectively. DSPy enables the creation of succinct programs that bootstrap state-of-the-art multi-stage NLP systems.

### Contributions

1. A systematic abstraction for building LM pipelines, replacing brittle prompt engineering.
2. An automated compiler that optimizes LM pipelines for both performance and efficiency.
3. Empirical evidence showing DSPy outperforms hand-crafted solutions while utilizing smaller LMs.

## The DSPy Programming Model

**DSPy** is a programming model designed to treat large language models (LMs) as abstract text-generation devices, optimizing their usage within computational graphs. DSPy programs are written in Python and handle tasks by processing inputs (e.g., questions or documents) through a series of optimized steps to produce outputs (e.g., answers or summaries). The framework introduces three key abstractions: **signatures**, **modules**, and **teleprompters**.

### Language Model Integration

- **Promptable LMs:** Capable of in-context learning (e.g., GPT-3.5 or Llama2-7b).
- **Finetunable LMs:** Smaller models like T5-base.
- A default LM can be configured, and DSPy will optimize operations accordingly unless otherwise specified.

### Key Abstractions in DSPy

1. **Signatures:** Natural-language typed function declarations that abstract the behavior of a task. Instead of writing hand-crafted prompts, signatures specify what needs to be done, not how.
2. **Modules:** Replace manual prompt engineering and can be composed into arbitrary pipelines. They encapsulate reusable logic and enable seamless integration of multiple LMs or task steps.
3. **Teleprompters:** Optimize modules within a pipeline to maximize performance metrics. They support techniques like cross-validation, reinforcement learning, or Bayesian hyperparameter optimization.

## DSPy Signatures

- A signature is a tuple of **input fields**, **output fields**, and optional **instructions**.
- Example: `qa = dspy.Predict("question -> answer")` then `qa(question="Where is Guaraní spoken?")` yields `Prediction(answer='Mainly in South America.')`.
- Fields like `question` and `answer` have semantic roles inferred by DSPy, guiding how LMs interpret them using in-context learning.
- Signatures provide compilation into high-quality prompts or finetunes and reduction of brittle string manipulation.

## Modules in DSPy

A **module** in DSPy implements a signature (e.g., the core `Predict` module). Modules store: the specified signature, an optional LM to override the default, and a list of demonstrations for prompting, initially empty. Modules act as callable functions, taking keyword arguments, formatting a prompt using the signature and demonstrations, calling the LM, and parsing output fields.

DSPy includes sophisticated modules like **ChainOfThought**, **ProgramOfThought**, **MultiChainComparison**, and **ReAct**, modularly implementing prompting techniques from recent research. These modules are interchangeable—e.g., replacing `Predict` with `ChainOfThought` results in a system that reasons step-by-step before generating an output.

### Parameterizing Modules

Modules are parameterized with: the specific LM to use, prompt instructions and string prefixes for signature fields, and demonstrations used as few-shot prompts (for frozen LMs) or as training data (for fine-tuning). The focus is on generating and selecting **useful demonstrations** to systematically teach LM pipelines new behaviors.

### Pipeline Composition

DSPy enables pipelines to be built using a **define-by-run interface**, inspired by PyTorch. Users declare required modules at initialization and build pipelines by defining how these modules interact in the `forward` method. For example, a **RAG** system: retrieve context with `dspy.Retrieve`, then generate an answer with `dspy.ChainOfThought("context, question -> answer")`. Replace `Predict` with `ChainOfThought` to enable step-by-step reasoning; change the signature to modify behavior. Built-in support for ColBERTv2, Pyserini, Pinecone retrievers, and experimental tools like `dspy.SQL` and `dspy.PythonInterpreter`.

## Teleprompters in DSPy

**Teleprompters** serve as optimizers that automate the process of optimizing programs for specific pipelines. They take a program, a training set, and a metric, and return an optimized version. They typically employ **gradient-free optimization strategies** and work efficiently even with small training sets. Labels are assumed primarily for the program's final output. Metrics can range from Exact Match (EM) or F1 to complex DSPy programs. Example: the RAG module can be optimized using a small dataset of question–answer pairs and an EM metric to bootstrap effective few-shot demonstrations via **BootstrapFewShot**. **BootstrapFinetune** can fine-tune on a larger, unlabeled dataset using a teacher program and the `answer_passage_match` metric to ensure grounded answers. Teleprompters allow **teacher-student compositions**.

## The DSPy Compiler

DSPy's strength lies in **automatically optimizing programs** via "compilation" using a **teleprompter**. Prompting and finetuning are unified under this optimization process.

### How Teleprompters Work

**Stage 1: Candidate Generation** — The teleprompter identifies all unique Predict modules, generates **candidates** (e.g., demonstrations), simulates the program on training inputs, tracks multi-stage traces, and uses the metric to filter bad traces. Good examples become potential demonstrations.

**Stage 2: Parameter Optimization** — After generating candidates, the teleprompter selects the best ones using random search, Tree-structured Parzen Estimators (HyperOpt, Optuna), or finetuning to update LM weights per predictor.

**Stage 3: Higher-Order Program Optimization** — The teleprompter may modify control flow (e.g., ensembling: multiple versions of the program run in parallel, predictions combined by majority voting). Future work could involve test-time bootstrapping or automated backtracking logic.

### Mathematical Tools

Teleprompters rely on **random search** and **Tree-structured Parzen Estimators (TPE)** for hyperparameter tuning. Ensembles and future optimizations allow DSPy to support dynamic workflows.

---

DSPy represents a significant advancement in AI system design by enabling efficient, modular, and optimized use of LMs. Its ability to handle a wide range of tasks and reduce reliance on large, hand-crafted prompts positions it as a valuable tool for developing high-performance AI systems with small LMs.
