---
title: Program of Thoughts Prompting - Disentangling Computation from Reasoning for Numerical Reasoning Tasks
tags:
  - AI
  - Machine Learning
  - NLP
publishedDate: "Nov 1, 2022"
sourceUrl: https://arxiv.org/abs/2211.12588
sourceLabel: arXiv
reviewedDate: "Jan 2025"
status: Complete
---

Numerical reasoning is a key challenge in AI, and many datasets have been created to test deep learning models' ability to solve math problems. The most common benchmarks involve **math word problems (MWP)**, where AI models answer math-related questions expressed in natural language. Some datasets also focus on **financial problems**, requiring models to handle financial calculations.

Earlier approaches trained models from scratch or fine-tuned them to generate step-by-step solutions. However, this required large amounts of **expert-annotated training data**. Recently, researchers found that **large language models (LLMs)** can solve these problems using a technique called **Chain-of-Thought (CoT) prompting**, where models are given a few examples with explanations and answers. This method has achieved **state-of-the-art results** on many reasoning tasks.

## In-Context Learning & Chain of Thought (CoT) Reasoning

**In-context learning** allows large language models (LLMs) to make predictions based on a few examples (prompts) without modifying their internal parameters. Instead of training on large datasets, LLMs learn from a few input-output demonstrations and then apply the same reasoning pattern to new problems.

A more advanced form of in-context learning is **Chain-of-Thought (CoT) prompting**, where the model is not just given input-output pairs but also a step-by-step explanation (rationale). This has been shown to improve LLMs' reasoning ability on various complex tasks.

**Limitations of Chain-of-Thought (CoT) Reasoning**

CoT relies on LLMs to both **reason and compute** the answers, but this has several problems:

1. **LLMs make calculation mistakes**, especially with large numbers.
2. **LLMs struggle with complex math**, like solving polynomial or differential equations.
3. **LLMs are inefficient for iterative processes**, especially when many steps are needed.

## Program of Thoughts (PoT) Prompting

Instead of expressing reasoning in **natural language**, PoT represents the thought process as a **Python program** with meaningful variable names. The model generates code that describes how to solve the problem, and then a **Python interpreter executes the code** to compute the final answer.

![How PoT works](/papers/2211-12588/Program%20of%20Thoughts%20Prompting_Disentangling%20Computation%20from%20Reasoning%20for%20Numerical%20Reasoning%20Tasks_1.png)

### How PoT Works

- Instead of solving math directly, the model **generates Python code** that describes the solution.
- The code is then executed by an external Python interpreter to get the correct answer.
- This allows PoT to handle **iterations, complex equations, and precise calculations** far better than CoT. For example:
  - **CoT struggles with solving a cubic equation**, whereas **PoT can generate Python code using the SymPy library** to solve it accurately.
  - **CoT performs poorly on long iterations**, but **PoT can express them in a few lines of code** and run them efficiently.

### Key Differences Between PoT and CoT

![PoT vs CoT](/papers/2211-12588/Program%20of%20Thoughts%20Prompting_Disentangling%20Computation%20from%20Reasoning%20for%20Numerical%20Reasoning%20Tasks_0.png)

- **CoT:** The LLM performs both reasoning and computation, which often leads to **errors in arithmetic and complex equations**.
- **PoT:** The LLM **only generates the reasoning steps** in code, while the computation is handled by Python, leading to **more accurate results**.

For example, instead of asking the model to directly generate and solve the equation:

$$\text{solve}(20000 \times (1 + x)^3 - 2000 - x \times 20000 \times 3 - 1000, x)$$

PoT **breaks the problem into multiple steps**, defining variables like `interest_rate` and `sum_in_two_years_with_XXX_interest`, making the process easier to follow and execute correctly.

## PoT as an Intermediate Step

Some problems require **both numerical computation and additional reasoning**. In such cases, PoT can be used as an **intermediate step** before applying CoT for further reasoning.

**How it Works:**

1. PoT generates a **Python program** to compute intermediate values.
2. If further reasoning is needed, CoT takes over to derive the final answer.
3. The model is taught when to **"keep prompting"** and use CoT after PoT.

For example, in a problem where two trains meet in **2.05 hours**, PoT can calculate this value correctly. However, if the answer requires converting **2.05 hours into HH:MM format**, CoT can be used to finalize the output.

![PoT + CoT](/papers/2211-12588/Program%20of%20Thoughts%20Prompting_Disentangling%20Computation%20from%20Reasoning%20for%20Numerical%20Reasoning%20Tasks_2.png)

This combined approach is particularly useful for datasets like **AQuA**, where answers need additional formatting, while most other datasets can be solved with PoT alone.

### Experimental Results

PoT was tested on **five MWP datasets** (GSM8K, AQuA, SVAMP, TabMWP, MultiArith) and **three financial datasets** (FinQA, ConvFinQA, TATQA). These datasets include different formats like **text, tables, and conversations**.

### Key Findings

- **PoT significantly outperforms CoT** across all datasets.
- **Few-shot setting:** PoT improves accuracy by **8% on MWP datasets** and **15% on financial datasets** compared to CoT.
- **Zero-shot setting:** PoT improves by **12% on MWP datasets** over CoT.
- **PoT + Self-Consistency (PoT+SC)** beats **CoT+SC** by an **average of 10%**, achieving the **best-known results** on MWP datasets and nearly the best on financial datasets (excluding GPT-4).

![Results](/papers/2211-12588/Program%20of%20Thoughts%20Prompting_Disentangling%20Computation%20from%20Reasoning%20for%20Numerical%20Reasoning%20Tasks_3.png)

## Summary of Experiments

### Experimental Setup

**Datasets Used:** The study evaluates **Program-of-Thoughts (PoT) prompting** on multiple datasets covering **math word problems (MWP) and financial reasoning**. The datasets include various input formats like **questions, tables, and conversations**. To handle different structures, tables are converted into text using a standard format, with columns separated by `|` and rows by `\n`.

**Implementation Details:**

- **Models Used:** The primary model is **Codex (code-davinci-002)**, but other models like **GPT-3, ChatGPT, CodeGen, CodeT5+, and Xgen** were tested for comparison.
- **Execution Method:** PoT-generated code is run using **Python 3.8 and the SymPy library** for precise calculations.
- **Few-Shot Learning:** The number of examples (shots) varies based on dataset difficulty—**fewer shots** for simple datasets like FinQA and **more shots (up to 8)** for challenging datasets like AQuA and TATQA.
- **Prompt Optimization:** LLMs sometimes generate reasoning chains in comments instead of Python code. To prevent this, a bias is applied to **suppress the '#' token**, encouraging models to output executable code.

**Evaluation Metrics:** Exact Match (EM) scores for MWP datasets; for AQuA, PoT computes the answer first then selects the closest multiple-choice option; financial datasets use official GitHub evaluation scripts; LLM errors in arithmetic are accounted for using `math.isclose` with relative tolerance 0.001.

**Baselines Compared:** PoT is compared against Codex, GPT-3, PaLM, LaMDA, Chain-of-Thought (CoT), CoT+External Calculator, and Self-Consistency (SC) decoding (majority vote over 40 outputs).

### Main Results

**Few-Shot Performance:** PoT outperforms CoT across all datasets. MWP improvements: +8% on GSM8K, AQuA, TabMWP; +4% on SVAMP. Financial QA: +20% on FinQA, ConvFinQA; +8% on TATQA. CoT struggles with large numbers; PoT uses Python for precise calculations. Even **CoT+calc** is outperformed by PoT because rigid post-processing in calculators often fails to correctly adjust the generated reasoning chain.

**Few-Shot + Self-Consistency:** PoT+SC still outperforms CoT+SC, especially on MWP datasets (+12% avg) and financial datasets (+20% on FinQA/ConvFinQA, +7% on TATQA).

**Zero-Shot Performance:** PoT significantly outperforms CoT; zero-shot PoT beats zero-shot CoT by an average of 12%. On TabMWP, zero-shot PoT even surpasses few-shot CoT. PoT generalizes better to unseen numerical reasoning tasks without needing dataset-specific examples.

### Ablation Studies

**Effect of Different Models:** GPT-3.5-turbo performs best. Text-based instruction tuning weakens Codex. Open-source models like CodeGen underperform.

**Sensitivity to Example Selection:** More shots help complex datasets (GSM8K) more than simpler ones (FinQA). With only 2 shots, PoT's performance can vary by up to 7%.

**Comparison with PaL:** PoT outperforms PaL (Program-aided Language), especially on SVAMP and ASDIV (+6% gain).

**Multi-Step Reasoning & Semantic Binding:** Breaking problems into step-by-step reasoning and using meaningful variable names (instead of generic `a, b, c`) improves performance. Direct equation generation is harder for LLMs than multi-step reasoning.

**Breakdown by Problem Type:** PoT significantly outperforms CoT in linear & polynomial equations, iterative problems, symbolic reasoning, and combinatorics. PoT performs similarly to CoT in basic arithmetic, probability, and geometry. More complex problems benefit from structured programming.

### Error Analysis

- **Value Grounding Errors (47%)** – Incorrect values assigned to variables.
- **Logic Errors (33%)** – Incorrect computation process even with correct variable assignments.
- **Combined Errors (15%)** – Both types together.
- **Correct Answers Misclassified (5%)** – Evaluation issues.

The majority of errors were value grounding issues, similar to CoT.

## Conclusion & Limitations

**Key Takeaways:** This work separates reasoning from computation using **Program-of-Thoughts (PoT) prompting**. LLMs can generate structured programs to express complex reasoning, delegate computation to an external Python interpreter, significantly outperform CoT across math and financial datasets, benefit from Self-Consistency decoding, achieve strong zero-shot generalization, and utilize semantic binding and multi-step reasoning for better problem decomposition.

**Limitations & Future Work:** Code execution risks—LLM-generated code could include harmful commands; PoT restricts imports to pre-approved modules, which limits generalization. PoT achieves only 58% accuracy on AQuA due to the wide variety of algebraic questions; future work should explore better prompting for more diverse math problems. By integrating symbolic execution with LLMs, this research paves the way for more reliable, scalable, and mathematically capable AI systems.
