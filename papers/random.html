<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Your Title</title>
    </head>
    <body>
<ul><li>Abstract</li><ul><li>Large language models (LLMs) usually reason in natural language, following a step-by-step process called chain-of-thought (CoT). However, this approach can be inefficient because many words are unnecessary for reasoning, and some steps are hard for the model to handle.</li><li>Coconut (Chain of Continuous Thought) is a new method that moves reasoning to a "latent space." Instead of turning thoughts into words, it keeps them as hidden representations and feeds them back into the model.</li><li>This allows Coconut to explore multiple solutions at once, like a breadth-first search, instead of sticking to one path. It’s better at complex reasoning tasks that need backtracking and uses fewer steps. Coconut shows how reasoning outside language can improve LLMs.</li></ul><li>Large language models (LLMs) are powerful at reasoning, thanks to extensive training on human language. However, because they rely on predicting the next word, their reasoning must be expressed in words, such as in the chain-of-thought (CoT) approach, which generates step-by-step solutions in natural language.</li><li>Human reasoning is different:</li><ul><li>As studies show that the brain’s language areas are mostly inactive during reasoning tasks.</li><li>Human language is designed for communication, not reasoning, and LLMs face challenges because some reasoning steps need complex planning, while most words are included just for fluency.</li><li>Yet, LLMs use the same amount of processing power for all tokens, leading to inefficiencies.</li></ul><li>Efforts to improve this, like creating shorter reasoning chains or adding extra reasoning steps for important tokens, still stay limited to language-based reasoning. <b>A better approach would allow LLMs to reason freely without relying on language and only convert their findings into words when needed.</b></li><li></li><li>This work introduces <b>Coconut (Chain of Continuous Thought)</b>, a new method for reasoning in LLMs that shifts from language-based reasoning to a "latent space." Instead of converting hidden states into words, Coconut directly uses the model’s hidden state (continuous thought) as input for the next step. This approach allows reasoning to happen outside the constraints of language and can be optimized end-to-end using gradient descent.</li><ul><li>expalian how instead of computing predicting the token for next time step, it feeds the hidden state as input to next steo</li></ul><li>To improve latent reasoning, Coconut uses a multi-stage training process that incorporates language reasoning chains for guidance. This leads to an efficient reasoning pattern: Coconut can explore multiple possible solutions at once, similar to a breadth-first search (BFS). It keeps various options open and gradually eliminates incorrect paths using implicit value functions, even without explicit training for this behavior.</li><ul><li>explain what is bfs</li></ul><li>Experiments show Coconut enhances LLM performance, especially in tasks requiring complex reasoning, such as math problems (GSM8k) and logical reasoning (ProntoQA, ProsQA). It outperforms traditional chain-of-thought (CoT) methods while generating fewer tokens, highlighting the potential of latent reasoning for solving harder problems and guiding future research.</li><li></li><li>CoT</li><ul><li>CoT reasoning refers to methods where models generate intermediate reasoning steps in language before providing a final answer. This can involve:</li><ul><li>Prompting models to reason step-by-step.</li><li>Training models with supervised fine-tuning or reinforcement learning to produce reasoning chains.</li></ul><li><b>Key Insights from Previous Work</b>:</li><ul><li>CoT tokens (symbols, patterns, and text) can be analyzed to guide models toward more concise reasoning.</li><li>CoT improves the model's expressiveness and depth by feeding outputs back into the input, effectively making reasoning "deeper."</li><li>However, the reliance on token-by-token (autoregressive) generation makes it difficult for CoT to handle complex problems requiring planning or search (e.g., tree-based reasoning).</li></ul><li><b>Alternatives and Extensions</b>:</li><ul><li>Some studies add explicit tree search algorithms or train models on search dynamics, enabling more flexible reasoning.</li><li>Removing the constraints of language space can result in a new reasoning pattern (similar to Breadth-First Search) without explicit training.</li></ul></ul><li>Latent Reasoning in LLMs</li><ul><li>Latent reasoning refers to the internal computations within large language models (LLMs) that happen in their hidden states, rather than the reasoning explicitly seen in their outputs. Key findings include:</li><li><b>Recovering Hidden Information</b>:</li><ul><li>Some studies have shown it’s possible to extract intermediate steps of reasoning (e.g., for multi-step problems) from a model’s hidden layers.</li><li>Researchers have also intervened in these hidden states to influence reasoning paths.</li></ul><li><b>Parallel Reasoning Paths</b>:</li><ul><li>Models can follow multiple hidden reasoning paths at once, even when generating a single chain of thought (CoT). However, the CoT outputs don’t always align with the actual internal reasoning. This is called "unfaithfulness" of CoT reasoning.</li></ul><li><b>Enhancing Latent Reasoning</b>:</li><ul><li>Adding special tokens (like <code><pause></code> or filler symbols such as "...") during training has improved task performance, especially for simpler problems. However, these methods don’t increase the model’s overall reasoning capability like CoT does.</li><li>Other methods, such as predicting a "planning token" or using knowledge distillation (training the model with complex reasoning outputs), help internalize reasoning into the hidden layers.</li></ul><li><b>Training Innovations</b>:</li><ul><li>Breaking down reasoning into smaller stages (inspired by iterative CoT) has been effective for training.</li><li>"Looped transformers," which feed outputs back into the model, show promise for solving algorithmic tasks, though their primary focus has been reasoning in hidden states rather than explicit outputs.</li></ul></ul><li>Coconut: Chain of Continuous Thought</li><ul><li><b>Background and Notation</b>:</li><ul><li>The input sequence <span>$x = (x_1, ..., x_T)$</span> is processed by a standard language model MM, where:</li><ul><li><span>$E_t = [e(x_1), e(x_2), ..., e(x_t)]$</span> is the sequence of token embeddings for the first tt tokens.</li><li><span>$H_t$</span> is the matrix of hidden states for all tokens up to position <span>$t$</span>.</li><li><span>$h_t$</span> is the hidden state for the token at position <span>$t$</span>, i.e., the last row of <span>$H_t$</span>.</li><li>The language model generates the next token <span>$x_{t+1}$</span> using the formula:<p>			  <span>$M(x_{t+1} | x_{\leq t}) = \text{softmax}(W h_t)$</span><br /></p></li><li><span>$e(\cdot)$</span> is the token embedding function, and <span>$W$</span> is the model's parameters.</li></ul></ul><li><b>Coconut Method Overview</b>:</li><ul><li><b>Switching Modes</b>: The Coconut method alternates between two modes:</li><ul><li><b>Language Mode</b>: The model works like a standard language model, generating the next token autoregressively.</li><li><b>Latent Mode</b>: The model uses the last hidden state as the input for the next token, not token embeddings. This hidden state represents the current reasoning state, referred to as a "continuous thought."</li></ul><li><b>Special Tokens</b>:</li><ul><li><code><bot></code> marks the start of the latent thought mode.</li><li><code><eot></code> marks the end of the latent thought mode.</li></ul><li><b>Latent Reasoning</b>:</li><ul><li>Assume latent reasoning happens between positions <span>$i$</span> and <span>$j$</span> (i.e., <span>$x_i = <bot>$</span> and <span>$x_j = <eot>$</span>).</li><li>In latent mode (when <span>$i < t < j$</span>), the model uses the hidden state from the previous token as the next input, replacing token embeddings:<p>			  <span>$E_t = [e(x_1), e(x_2), ..., e(x_i), h_i, h_{i+1}, ..., h_{t-1}]$</span><br /></p></li></ul><li><span></span></li><li><b>Reverting to Language Mode</b>: After the latent mode ends (when <span>$\geq j$</span>), the model reverts back to using token embeddings as input:<p>		  <span>$E_t = [e(x_1), e(x_2), ..., e(x_i), h_i, h_{i+1}, ..., h_{j-1}, e(x_j), ..., e(x_t)]$</span><br /></p></li><li><b>Hidden States</b>: The hidden states are processed by the final normalization layer, ensuring they don’t have too large magnitudes.</li><li><b>Model Behavior</b>: During the latent mode (i < t < j), the model does not map the latent thought back to language space. However, it can still compute <span>$\text{softmax}(W h_t)$</span> for analysis purposes.</li></ul></ul><li></li><li></li><li></li><li>Test</li><ul><li>Summary of the Coconut Paradigm<p>	  <br />	  In this section, we present a new approach called Coconut (Chain of Continuous Thought) for reasoning in a flexible way within a language model. Here’s a simplified breakdown:<br /></p></li><li>Background and Notation</li><li><b>Input Sequence</b>: We denote an input sequence as <span>$$ x = (x_1, ..., x_T) $$</span>.</li><li><b>Language Model Description</b>: A large language model (LLM) is represented as:<span></span>
	  H_t = \text{Transformer}(E_t)
	  </span><span>
	  M(x_{t+1} | x_{\leq t}) = \text{softmax}(W h_t)
	  </span><p>	  where:<br /></p></li><ul><li><span> E_t = [e(x_1), e(x_2), ..., e(x_t)] </span><p>: This is the list of token embeddings up to position <span>$$ t $$</span>.<br /></p></li><li><span> H_t \in \mathbb{R}^{t \times d} </span><p>: This matrix contains the hidden states for all tokens up to position <span>$$ t $$</span>.<br /></p></li><li><span> h_t </span><p>: The hidden state for the token at position <span>$$ t $$</span>.<br /></p></li><li><span> e(\cdot) </span><p>: A function that converts tokens into embeddings.<br /></p></li><li><span> W </span><p>: Parameters for the language model head.<br /></p></li></ul><li>Overview of the Coconut Method<p>	  <br />	  The Coconut method allows the model to switch between two modes:<br />	  <br /></p></li><ul><li><p><b>Language Mode</b>: In this mode, the model behaves like a typical language model, generating the next token based on previous tokens.<br /></p></li><li><p><b>Latent Mode</b>: Here, the model uses the last hidden state as the input for generating the next token. This hidden state reflects the current reasoning process, which we call "continuous thought."</p></li></ul><li>Special Tokens</li><li><b><bot></b>: Marks the start of latent reasoning.</li><li><b><eot></b>: Marks the end of latent reasoning.</li><li>Example of Latent Reasoning<p>	  <br />	  If latent reasoning occurs between positions <span>$$ i $$</span> and <span>$$ j $$</span> (where <span>$$ x_i = <bot> $$</span> and <span>$$ x_j = <eot> $$</span>), during latent mode (when <span>$$ i < t < j $$</span>), we replace the input embedding with the last hidden state from the previous token:<br /></p><span>
	  E_t = [e(x_1), e(x_2), ..., e(x_i), h_i, h_{i+1}, ..., h_{t-1}]
	  </span><p>	  <br />	  After exiting latent mode (when <span>$$ t \geq j $$</span>), we revert to using token embeddings:<br /></p><span>
	  E_t = [e(x_1), e(x_2), ..., e(x_i), h_i, h_{i+1}, ..., h_{j-1}, e(x_j), ..., e(x_T)]
	  </span></li><li>Important Note<p>	  <br />	  While in latent mode, we do not define <span>$$ M(x_{t+1} | x_{\leq t}) $$</span> because this thought process does not map back to language. However, we can still calculate <span>$$ \text{softmax}(W h_t) $$</span> for analysis purposes.<br />	  <br />	  This new paradigm enhances how language models can reason and generate responses by incorporating a continuous thought process.<br /></p></li></ul><li></li><li></li></ul>
</body>
      </html>